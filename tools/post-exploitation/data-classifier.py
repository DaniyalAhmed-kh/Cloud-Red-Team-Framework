#!/usr/bin/env python3
"""
Cloud Data Classifier & Discovery Tool
Identifies sensitive data across cloud storage and databases
"""

import re
import json
import argparse
import sys
from typing import Dict, List, Tuple
from dataclasses import dataclass
import hashlib


@dataclass
class DataClassification:
    """Data classification result"""
    data_type: str
    confidence: float
    location: str
    sample: str
    risk_level: str  # LOW, MEDIUM, HIGH, CRITICAL


class DataClassifier:
    """Classify and identify sensitive data patterns"""
    
    # Classification patterns
    PATTERNS = {
        'credit_card': {
            'regex': r'(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13})',
            'risk': 'CRITICAL',
            'confidence': 0.95
        },
        'ssn': {
            'regex': r'\b(?:XXX|\d{3})-\d{2}-\d{4}\b',
            'risk': 'CRITICAL',
            'confidence': 0.90
        },
        'email': {
            'regex': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            'risk': 'MEDIUM',
            'confidence': 0.85
        },
        'phone': {
            'regex': r'(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
            'risk': 'MEDIUM',
            'confidence': 0.80
        },
        'api_key': {
            'regex': r'(?i)api[_-]?key[:\s=]+[a-z0-9]{20,}',
            'risk': 'CRITICAL',
            'confidence': 0.92
        },
        'aws_access_key': {
            'regex': r'AKIA[0-9A-Z]{16}',
            'risk': 'CRITICAL',
            'confidence': 0.99
        },
        'private_key': {
            'regex': r'-----BEGIN (?:RSA|DSA|EC) PRIVATE KEY-----',
            'risk': 'CRITICAL',
            'confidence': 0.99
        },
        'password': {
            'regex': r'(?i)(?:password|passwd|pwd)[:\s=]+[^\s\"]{8,}',
            'risk': 'CRITICAL',
            'confidence': 0.85
        },
        'bearer_token': {
            'regex': r'(?i)(?:bearer|authorization)[:\s=]+[a-z0-9_.=-]{20,}',
            'risk': 'CRITICAL',
            'confidence': 0.90
        },
        'ipv4': {
            'regex': r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b',
            'risk': 'LOW',
            'confidence': 0.70
        },
        'database_connection': {
            'regex': r'(?:mongodb|postgresql|mysql|mssql)://[a-z0-9:/@.-]+',
            'risk': 'CRITICAL',
            'confidence': 0.88
        },
        'pii_name': {
            'regex': r'\b(?:John|Jane|Michael|Mary|David|Lisa|Robert|Susan|James|Jessica)\s+(?:Smith|Johnson|Williams|Brown|Jones|Garcia|Miller)\b',
            'risk': 'MEDIUM',
            'confidence': 0.60
        }
    }
    
    # Column name patterns that indicate sensitive data
    SENSITIVE_COLUMNS = [
        'password', 'pwd', 'passwd', 'secret', 'token', 'api_key',
        'credit_card', 'card_number', 'cc_number', 'cvv', 'ssn',
        'social_security', 'driver_license', 'license_number',
        'bank_account', 'account_number', 'routing_number',
        'email', 'phone', 'mobile', 'salary', 'income',
        'dob', 'date_of_birth', 'birthday', 'address',
        'encrypted_password', 'hash', 'secret_key'
    ]
    
    def __init__(self):
        self.classifications: List[DataClassification] = []
        self.compiled_patterns = {}
        self._compile_patterns()
    
    def _compile_patterns(self):
        """Pre-compile regex patterns for performance"""
        for name, pattern_info in self.PATTERNS.items():
            self.compiled_patterns[name] = re.compile(pattern_info['regex'], re.IGNORECASE)
    
    def classify_text(self, text: str, location: str = "unknown") -> List[DataClassification]:
        """Classify text for sensitive data"""
        findings = []
        
        if not isinstance(text, str) or len(text) == 0:
            return findings
        
        # Truncate for performance
        text = text[:100000]
        
        for data_type, pattern_info in self.PATTERNS.items():
            matches = self.compiled_patterns[data_type].finditer(text)
            
            for match in matches:
                sample = match.group(0)[:100]  # Truncate sample
                
                finding = DataClassification(
                    data_type=data_type,
                    confidence=pattern_info['confidence'],
                    location=location,
                    sample=sample,
                    risk_level=pattern_info['risk']
                )
                
                findings.append(finding)
        
        return findings
    
    def classify_column_name(self, column_name: str) -> Tuple[str, float]:
        """Classify based on column name"""
        col_lower = column_name.lower()
        
        for sensitive in self.SENSITIVE_COLUMNS:
            if sensitive in col_lower:
                return (sensitive, 0.85)
        
        return (None, 0.0)
    
    def classify_s3_object(self, bucket: str, key: str, content: str) -> Dict:
        """Classify S3 object"""
        classification = {
            'bucket': bucket,
            'key': key,
            'sensitive_data': [],
            'risk_level': 'LOW'
        }
        
        # Analyze key name
        key_findings, key_confidence = self.classify_column_name(key)
        if key_findings:
            classification['sensitive_data'].append({
                'type': 'filename_pattern',
                'pattern': key_findings,
                'confidence': key_confidence
            })
        
        # Analyze content
        findings = self.classify_text(content, f"s3://{bucket}/{key}")
        
        for finding in findings:
            classification['sensitive_data'].append({
                'type': finding.data_type,
                'confidence': finding.confidence,
                'sample': finding.sample,
                'risk': finding.risk_level
            })
        
        # Determine overall risk
        if any(f['risk'] == 'CRITICAL' for f in classification['sensitive_data'] if 'risk' in f):
            classification['risk_level'] = 'CRITICAL'
        elif any(f['risk'] == 'HIGH' for f in classification['sensitive_data'] if 'risk' in f):
            classification['risk_level'] = 'HIGH'
        elif any(f['risk'] == 'MEDIUM' for f in classification['sensitive_data'] if 'risk' in f):
            classification['risk_level'] = 'MEDIUM'
        
        return classification
    
    def classify_database_record(self, table_name: str, record: Dict) -> Dict:
        """Classify database record"""
        classification = {
            'table': table_name,
            'sensitive_columns': [],
            'risk_level': 'LOW'
        }
        
        for column, value in record.items():
            if not isinstance(value, str):
                value = str(value)
            
            # Check column name
            col_type, col_confidence = self.classify_column_name(column)
            
            if col_type:
                classification['sensitive_columns'].append({
                    'column': column,
                    'reason': col_type,
                    'confidence': col_confidence
                })
            
            # Check value
            findings = self.classify_text(value, f"{table_name}.{column}")
            
            for finding in findings:
                classification['sensitive_columns'].append({
                    'column': column,
                    'data_type': finding.data_type,
                    'confidence': finding.confidence,
                    'sample': finding.sample,
                    'risk': finding.risk_level
                })
        
        # Determine overall risk
        risks = [c.get('risk') for c in classification['sensitive_columns'] if 'risk' in c]
        if 'CRITICAL' in risks:
            classification['risk_level'] = 'CRITICAL'
        elif 'HIGH' in risks:
            classification['risk_level'] = 'HIGH'
        elif 'MEDIUM' in risks:
            classification['risk_level'] = 'MEDIUM'
        
        return classification
    
    def generate_report(self, classifications: List[Dict]) -> str:
        """Generate human-readable report"""
        report = []
        report.append("=" * 80)
        report.append("DATA CLASSIFICATION REPORT")
        report.append("=" * 80)
        report.append("")
        
        # Summary
        critical = len([c for c in classifications if c.get('risk_level') == 'CRITICAL'])
        high = len([c for c in classifications if c.get('risk_level') == 'HIGH'])
        medium = len([c for c in classifications if c.get('risk_level') == 'MEDIUM'])
        low = len([c for c in classifications if c.get('risk_level') == 'LOW'])
        
        report.append(f"SUMMARY")
        report.append(f"  CRITICAL: {critical}")
        report.append(f"  HIGH:     {high}")
        report.append(f"  MEDIUM:   {medium}")
        report.append(f"  LOW:      {low}")
        report.append("")
        
        # Critical findings
        if critical > 0:
            report.append("CRITICAL FINDINGS:")
            for c in classifications:
                if c.get('risk_level') == 'CRITICAL':
                    if 'key' in c:  # S3 object
                        report.append(f"  S3: {c['bucket']}/{c['key']}")
                    else:  # Database record
                        report.append(f"  Table: {c['table']}")
                    
                    for item in c.get('sensitive_data', [])[:3]:
                        report.append(f"    - {item.get('type')}: {item.get('sample', '')[:40]}")
                    report.append("")
        
        return "\n".join(report)
    
    def scan_s3_bucket(self, bucket: str, max_objects: int = 100) -> List[Dict]:
        """Scan S3 bucket for sensitive data"""
        try:
            import boto3
            s3 = boto3.client('s3')
            
            results = []
            count = 0
            
            print(f"[*] Scanning S3 bucket: {bucket}")
            
            paginator = s3.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=bucket, MaxKeys=max_objects):
                for obj in page.get('Contents', []):
                    if count >= max_objects:
                        break
                    
                    key = obj['Key']
                    print(f"  [*] Analyzing: {key}")
                    
                    try:
                        response = s3.get_object(Bucket=bucket, Key=key)
                        content = response['Body'].read().decode('utf-8', errors='ignore')
                        
                        classification = self.classify_s3_object(bucket, key, content)
                        
                        if classification['sensitive_data']:
                            results.append(classification)
                            print(f"    [+] Risk: {classification['risk_level']}")
                    
                    except Exception as e:
                        print(f"    [-] Error: {e}")
                    
                    count += 1
            
            return results
        
        except Exception as e:
            print(f"[-] Error scanning bucket: {e}")
            return []
    
    def scan_database(self, table_data: List[Dict], table_name: str) -> List[Dict]:
        """Scan database records for sensitive data"""
        results = []
        
        print(f"[*] Scanning table: {table_name}")
        
        for record in table_data[:1000]:  # Limit to 1000 records
            classification = self.classify_database_record(table_name, record)
            
            if classification['sensitive_columns']:
                results.append(classification)
        
        return results


def main():
    parser = argparse.ArgumentParser(
        description='Cloud Data Classifier & Discovery Tool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Classify text from file
  %(prog)s --classify-text data.txt
  
  # Scan S3 bucket
  %(prog)s --scan-s3 my-bucket --max-objects 100
  
  # Generate classification report
  %(prog)s --classify-text data.txt --report output.html
        '''
    )
    
    parser.add_argument('--classify-text', help='Classify text from file')
    parser.add_argument('--scan-s3', help='Scan S3 bucket')
    parser.add_argument('--max-objects', type=int, default=100, help='Max objects to scan')
    parser.add_argument('--report', help='Output report file')
    parser.add_argument('--json', action='store_true', help='Output as JSON')
    
    args = parser.parse_args()
    
    classifier = DataClassifier()
    results = []
    
    if args.classify_text:
        print(f"[*] Classifying text from: {args.classify_text}")
        
        with open(args.classify_text, 'r') as f:
            content = f.read()
        
        findings = classifier.classify_text(content, args.classify_text)
        
        print(f"\n[+] Found {len(findings)} sensitive data patterns:\n")
        
        for finding in findings:
            print(f"  Type: {finding.data_type}")
            print(f"  Risk: {finding.risk_level}")
            print(f"  Confidence: {finding.confidence * 100:.1f}%")
            print(f"  Sample: {finding.sample}")
            print()
    
    elif args.scan_s3:
        results = classifier.scan_s3_bucket(args.scan_s3, args.max_objects)
        
        print(f"\n[+] Classification complete: {len(results)} sensitive objects found")
    
    if args.report:
        report = classifier.generate_report(results)
        with open(args.report, 'w') as f:
            f.write(report)
        print(f"[+] Report written to: {args.report}")
    
    if args.json:
        import json
        print(json.dumps(results, indent=2, default=str))


if __name__ == '__main__':
    main()
